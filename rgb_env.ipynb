{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import pygame\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "import numpy as np\n",
    "import random, time\n",
    "import os\n",
    "#from stable_baseline3.stable_baselines3.ppo import ppo\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "#VISITED = [255, 255, 255]\n",
    "#UNVISITED = [255, 255, 0]\n",
    "#PLAYER = [255, 0, 0]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.zeros((3, 4, 5))\n",
    "print(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([[0,0,1,0,1,0,1,0,0,1,1,0,1,0,0,0],\n",
    "[0,0,1,0,1,0,1,1,0,1,0,0,1,0,0,1],\n",
    "[1,1,0,0,1,0,0,0,1,1,0,0,0,1,0,0],\n",
    "[0,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0],\n",
    "[0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,0],\n",
    "[1,1,0,0,1,0,0,0,0,0,1,1,1,0,0,0],\n",
    "[1,0,1,0,0,1,0,0,1,0,0,0,0,1,1,0],\n",
    "[0,1,1,0,0,0,1,0,0,0,1,0,1,0,0,0],\n",
    "[0,0,0,1,1,1,0,0,0,0,1,0,1,0,0,0],\n",
    "[1,0,0,1,0,1,0,1,0,1,1,0,1,0,1,0],\n",
    "[1,0,0,0,1,1,1,0,1,0,0,0,0,0,1,0],\n",
    "[0,1,0,0,1,0,1,1,1,0,1,1,0,0,0,0],\n",
    "[1,0,0,1,1,0,0,1,0,1,0,0,0,0,1,0],\n",
    "[1,0,0,1,0,0,1,0,1,0,1,1,1,1,0,0],\n",
    "[1,0,1,0,1,1,0,1,0,1,0,0,1,0,0,0],\n",
    "[1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2311444133449163\n"
     ]
    }
   ],
   "source": [
    "print(np.power(2, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "list = np.array([[1, 2], [3, 4]])\n",
    "print(list.shape)\n",
    "print(list[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grass_data = [[0,1,1,0,1,0,1,0,0,1,1,0,1,0,1,0,1,1,0,0],\n",
    "[1,1,1,0,1,0,1,1,0,1,0,0,1,0,0,1,0,1,1,0],\n",
    "[1,1,0,0,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,0],\n",
    "[0,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0,0,0,1,0],\n",
    "[0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0],\n",
    "[1,1,0,0,1,0,0,0,0,0,1,1,1,0,0,1,0,1,0,0],\n",
    "[1,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,0,1],\n",
    "[0,1,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0],\n",
    "[0,0,0,1,1,1,0,0,0,0,1,0,1,0,0,0,1,0,0,0],\n",
    "[1,0,0,1,0,1,0,1,0,1,1,0,1,0,1,0,1,0,0,0],\n",
    "[1,0,0,0,1,1,1,0,1,0,0,0,0,0,1,0,1,1,0,0],\n",
    "[0,1,0,0,1,0,1,1,1,0,1,1,0,0,0,1,0,0,0,1],\n",
    "[1,0,0,1,1,0,0,1,0,1,0,0,0,0,1,0,1,0,0,1],\n",
    "[1,0,0,1,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,1],\n",
    "[1,0,0,0,1,1,0,1,0,1,0,0,1,0,0,1,1,1,0,0],\n",
    "[0,0,1,1,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0],\n",
    "[1,1,1,0,0,0,1,0,0,0,0,1,1,0,1,0,0,1,1,0],\n",
    "[0,0,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,1,0],\n",
    "[0,1,0,0,1,0,1,1,0,0,0,0,1,0,0,1,1,1,0,0],\n",
    "[0,1,1,0,0,1,0,0,0,1,1,1,0,1,1,1,1,0,0,0]]\n",
    "walls_data = [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1],\n",
    "[0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,0,1,1,0,0],\n",
    "[0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0],\n",
    "[0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0],\n",
    "[0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1],\n",
    "[0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0],\n",
    "[0,0,0,1,0,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0],\n",
    "[0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0],\n",
    "[0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,1,0,0,1,1],\n",
    "[0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0],\n",
    "[0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,1,0,1,0],\n",
    "[0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0],\n",
    "[0,0,1,0,1,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,1],\n",
    "[0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1],\n",
    "[0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0],\n",
    "[0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0],\n",
    "[0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1],\n",
    "[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]\n",
    "corners = [[0, 0], [1, 1], [1, 0], [0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MowerEnv(Env):\n",
    "    def __init__(self, len, render_mode = True):\n",
    "        self.render_mode = render_mode\n",
    "        self.len = len\n",
    "        if render_mode == True:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((720, 720))\n",
    "        # Actions we can up down left right\n",
    "        self.action_space = Discrete(16)\n",
    "        # 0=visited, 1=notvisited\n",
    "        #gym.spaces.space.Space.seed(1)\n",
    "\n",
    "\n",
    "        self.observation_space = Dict({'grid': Box(low=0, high=1, shape=(2,self.len,self.len), dtype=np.uint8),                       #pos, bounds, guide\n",
    "                                    'all': Box(low=0, high=self.len-1, shape=(20,), dtype=np.uint8)})\n",
    "\n",
    "        # Set length\n",
    "        self.direct = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n",
    "\n",
    "    def step(self, action):\n",
    "        play = [action//4, action%4]\n",
    "        reward = 0\n",
    "        arr = self.state['grid'][0]\n",
    "        all = self.state['all']\n",
    "        for j in range(2):\n",
    "            # Apply action (no walls for now)\n",
    "            x = all[0 + j * 10] + self.direct[play[j]][0]\n",
    "            y = all[1 + j * 10] + self.direct[play[j]][1]\n",
    "            if self.isValid(x, y) == 0:\n",
    "                x = all[0 + j * 10]\n",
    "                y = all[1 + j * 10]\n",
    "            if arr[x][y] == 1:\n",
    "                self.area_clear += 1\n",
    "                reward = 5.0 * np.power(4, self.area_clear/self.max_area)\n",
    "            else:\n",
    "                reward = -0.5\n",
    "                self.running_length -= 1\n",
    "            arr[x][y] = 0\n",
    "            all[0+j*10] = x\n",
    "            all[1+j*10] = y\n",
    "\n",
    "            # update boundries\n",
    "            for i in range(4):\n",
    "                all[i+2+j*10] = self.isValid(x + self.direct[i][0], y + self.direct[i][1])\n",
    "            all[6+j*10], all[7+j*10], all[8+j*10], all[9+j*10] = self.leastDist(x, y)\n",
    "            # Check if  done\n",
    "            if self.running_length <= 0:\n",
    "                done = True\n",
    "                break\n",
    "            else:\n",
    "                done = False\n",
    "            if self.finished(): #change back to self.finished()\n",
    "                reward = self.max_area * 20\n",
    "                done = True\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return (self.state, reward, done, False, info)\n",
    "\n",
    "    def isValid(self, x, y):\n",
    "        if x < 0 or x < 0 or x >= self.len or y < 0 or y >= self.len:\n",
    "            return 0 #return False\n",
    "        if self.state['grid'][1][x][y] == 1:\n",
    "            return 0\n",
    "        return 1 #return true\n",
    "\n",
    "    def leastDist(self, x, y):\n",
    "        arr = self.state['grid'][0]\n",
    "        up = right = down = left = 0\n",
    "        done = False\n",
    "        for i in range(1, 11):\n",
    "            for j in range(-i, i+1):\n",
    "                k = i - abs(j)\n",
    "                if x + j >=0 and x + j < self.len and y + k < self.len:\n",
    "                    if arr[x+j][y+k] == 1:\n",
    "                        done = True\n",
    "                        down = max(int(k > 0), down)\n",
    "                        left = max(int(j < 0), left)\n",
    "                        right = max(int(j > 0), right)\n",
    "                if x + j >=0 and x + j < self.len and y - k >=0:\n",
    "                    if arr[x+j][y-k] == 1:\n",
    "                        done = True\n",
    "                        up = max(int(k > 0), up)\n",
    "                        left = max(int(j < 0), left)\n",
    "                        right = max(int(j > 0), right)\n",
    "            if done:\n",
    "                break\n",
    "        return (up, right, down, left)\n",
    "\n",
    "    def finished(self):\n",
    "        return self.area_clear==self.max_area\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        if self.render_mode == True:\n",
    "            pygame.event.get()\n",
    "            gap = 720//self.len\n",
    "            arr = self.state['grid'][0]\n",
    "            pos = self.state['all']\n",
    "            walls = self.state['grid'][1]\n",
    "            for i in range(0, 720, gap):\n",
    "                for j in range(0, 720, gap):\n",
    "                    if arr[i//gap][j//gap] == 0:\n",
    "                        color = (255, 255, 255)\n",
    "                    elif arr[i//gap][j//gap] == 1:\n",
    "                        color = (255, 255,0)\n",
    "                    if (i//gap == pos[0] and j//gap == pos[1]) or (i//gap == pos[10] and j//gap == pos[11]):\n",
    "                        color = (255, 0, 0)\n",
    "                    if walls[i//gap][j//gap] == 1:\n",
    "                        color = (0, 0, 0)\n",
    "                    pygame.draw.rect(self.screen, color, (j, i, gap, gap))\n",
    "            pygame.display.update()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Set starting state\n",
    "        self.state = self.observation_space.sample()\n",
    "        \"\"\"\n",
    "        image = self.state['grid']\n",
    "        walls = self.state['walls']\n",
    "        all = self.state['all']\n",
    "        for i in range(self.len):\n",
    "            for j in range(self.len):\n",
    "                image[i][j] = 1\n",
    "                walls[i][j] = 0\n",
    "        self.max_area = self.len * self.len-1 #change later\n",
    "        step = 2\n",
    "        for i in range(1, self.len-step+1, step):\n",
    "            for j in range(1, self.len-step+1, step):\n",
    "                x = i + random.randint(0, step) #wall generation\n",
    "                y = j + random.randint(0, step)\n",
    "                image[x][y] = 0\n",
    "                walls[x][y] = 1\n",
    "                self.max_area -= 1\n",
    "                \n",
    "        for i in range(0, self.len-step, step):\n",
    "            for j in range(0, self.len-step, step):        \n",
    "                x = i + random.randint(0, step)#blank generation\n",
    "                y = j + random.randint(0, step)\n",
    "                if image[x][y] == 1:\n",
    "                    self.max_area -= 1\n",
    "                image[x][y] = 0\n",
    "                x = i + random.randint(0, step)#blank generation\n",
    "                y = j + random.randint(0, step)\n",
    "                if image[x][y] == 1:\n",
    "                    self.max_area -= 1\n",
    "                image[x][y] = 0\n",
    "        \"\"\"\n",
    "        self.state['grid'][0] = np.array(grass_data, dtype = np.uint8)\n",
    "        self.state['grid'][1] = np.array(walls_data, dtype = np.uint8)\n",
    "        all = self.state['all']\n",
    "        all[0] = all[1] = 0\n",
    "        # Set length\n",
    "        self.running_length = (self.len)*(self.len)\n",
    "        all[2] = all[5] = 0\n",
    "        all[3] = all[4] = 1\n",
    "        all[6], all[7], all[8], all[9] = self.leastDist(all[0], all[1])\n",
    "        \n",
    "        all[0+10] = all[1+10] = self.len-1\n",
    "        # Set length\n",
    "        self.running_length = (self.len)*(self.len)\n",
    "        all[2+10] = all[5+10] = 1\n",
    "        all[3+10] = all[4+10] = 0\n",
    "        all[6+10], all[7+10], all[8+10], all[9+10] = self.leastDist(all[0+10], all[1+10])                \n",
    "        self.area_clear = 0\n",
    "        self.max_area = np.sum(self.state['grid'][0])\n",
    "        return self.state, {}\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env for PPO + normal (optimize later by making symmetry redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MowerEnv2(Env):\n",
    "    def __init__(self, info, render_mode = True): #info 0 = len, 1 = number of agents (up to 4)\n",
    "        self.render_mode = render_mode\n",
    "        self.len = info[0]\n",
    "        self.agents = info[1]\n",
    "        if render_mode == True:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((720, 720))\n",
    "        # Actions we can up down left right\n",
    "        self.action_space = MultiDiscrete([4 for i in range(self.agents)])\n",
    "        # 0=visited, 1=notvisited\n",
    "        #gym.spaces.space.Space.seed(1)\n",
    "\n",
    "\n",
    "        self.observation_space = Dict({'grid': Box(low=0, high=1, shape=(2,self.len,self.len), dtype=np.uint8),                       #pos, bounds, guide\n",
    "                                    'all': Box(low=0, high=self.len-1, shape=(10 * self.agents,), dtype=np.uint8)})\n",
    "\n",
    "        # Set length\n",
    "        self.direct = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        arr = self.state['grid'][0]\n",
    "        all = self.state['all']\n",
    "        for j in range(self.agents):\n",
    "            # Apply action (no walls for now)\n",
    "            x = all[0 + j * 10] + self.direct[action[j]][0]\n",
    "            y = all[1 + j * 10] + self.direct[action[j]][1]\n",
    "            if self.isValid(x, y) == 0:\n",
    "                x = all[0 + j * 10]\n",
    "                y = all[1 + j * 10]\n",
    "            if arr[x][y] == 1:\n",
    "                self.area_clear += 1\n",
    "                reward += 5.0 * np.power(4, self.area_clear/self.max_area)\n",
    "            else:\n",
    "                reward += -0.5\n",
    "                self.running_length -= 1\n",
    "            arr[x][y] = 0\n",
    "            all[0+j*10] = x\n",
    "            all[1+j*10] = y\n",
    "\n",
    "            # update boundries\n",
    "            for i in range(4):\n",
    "                all[i+2+j*10] = self.isValid(x + self.direct[i][0], y + self.direct[i][1])\n",
    "            temp = self.cheats(x, y)\n",
    "            all[6+j*10]= all[7+j*10]= all[8+j*10]= all[9+j*10] = 0\n",
    "            for p in temp:\n",
    "                all[6+j*10+p] = 1\n",
    "            # Check if  done\n",
    "            if self.running_length < 0:\n",
    "                done = True\n",
    "                break\n",
    "            else:\n",
    "                done = False\n",
    "            if self.finished(): #change back to self.finished()\n",
    "                print('joyce qu')\n",
    "                reward += 20 * self.max_area\n",
    "                done = True\n",
    "                break\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return (self.state, reward, done, False, info)\n",
    "\n",
    "    def isValid(self, x, y):\n",
    "        if x < 0 or x < 0 or x >= self.len or y < 0 or y >= self.len:\n",
    "            return 0 #return False\n",
    "        if self.state['grid'][1][x][y] == 1:\n",
    "            return 0\n",
    "        return 1 #return true\n",
    "\n",
    "    def cheats(self, posx, posy):\n",
    "        if np.sum(self.state['grid'][0]) == 0:\n",
    "            print('lesgo')\n",
    "            return [0, 1, 2, 3]\n",
    "        # Next action:\n",
    "        # (feed the observation to your agent here)\n",
    "        made = False\n",
    "        calc = np.zeros((20, 20))\n",
    "        list = deque()\n",
    "        calc[posx][posy] = 1\n",
    "        list.append([posx, posy])\n",
    "        move = []\n",
    "        while True: #has multiple paths issues, but should be irrelevent\n",
    "            size = len(list)\n",
    "            for z in range(size):\n",
    "                tuple = list.popleft()\n",
    "                for i in range(4):\n",
    "                    x = tuple[0] + self.direct[i][0]\n",
    "                    y = tuple[1] + self.direct[i][1]\n",
    "                    if x >= 0 and x < 20 and y >=0 and y < 20:\n",
    "                        if self.state['grid'][1][x][y] == 0 and calc[x][y] == 0:\n",
    "                            calc[x][y] = 1\n",
    "                            choice = i\n",
    "                            if len(tuple) == 3:\n",
    "                                choice = tuple[2]\n",
    "                            if self.state['grid'][0][x][y] == 0:\n",
    "                                ret = [x, y, choice]\n",
    "                                list.append(ret)\n",
    "                            else:\n",
    "                                move.append(choice)\n",
    "                                break\n",
    "\n",
    "            if len(move) > 0:\n",
    "                break\n",
    "        return move\n",
    "\n",
    "    def leastDist(self, x, y):\n",
    "        arr = self.state['grid'][0]\n",
    "        up = right = down = left = 0\n",
    "        done = False\n",
    "        for i in range(1, 11):\n",
    "            for j in range(-i, i+1):\n",
    "                k = i - abs(j)\n",
    "                if x + j >=0 and x + j < self.len and y + k < self.len:\n",
    "                    if arr[x+j][y+k] == 1:\n",
    "                        done = True\n",
    "                        down = max(int(k > 0), down)\n",
    "                        left = max(int(j < 0), left)\n",
    "                        right = max(int(j > 0), right)\n",
    "                if x + j >=0 and x + j < self.len and y - k >=0:\n",
    "                    if arr[x+j][y-k] == 1:\n",
    "                        done = True\n",
    "                        up = max(int(k > 0), up)\n",
    "                        left = max(int(j < 0), left)\n",
    "                        right = max(int(j > 0), right)\n",
    "            if done:\n",
    "                break\n",
    "        return (up, right, down, left)\n",
    "\n",
    "    def finished(self):\n",
    "        return self.area_clear==self.max_area\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        if self.render_mode == True:\n",
    "            pygame.event.get()\n",
    "            gap = 720//self.len\n",
    "            arr = self.state['grid'][0]\n",
    "            pos = self.state['all']\n",
    "            walls = self.state['grid'][1]\n",
    "            for i in range(0, 720, gap):\n",
    "                for j in range(0, 720, gap):\n",
    "                    if arr[i//gap][j//gap] == 0:\n",
    "                        color = (255, 255, 255)\n",
    "                    elif arr[i//gap][j//gap] == 1:\n",
    "                        color = (255, 255,0)\n",
    "                    for k in range(self.agents):\n",
    "                        if pos[0+k*10] == i//gap and pos[1+k*10] == j//gap:\n",
    "                            color = (255, 0, 0)\n",
    "                    if walls[i//gap][j//gap] == 1:\n",
    "                        color = (0, 0, 0)\n",
    "                    pygame.draw.rect(self.screen, color, (j, i, gap, gap))\n",
    "            pygame.display.update()\n",
    "\n",
    "    def perform_sort(self):\n",
    "        arr = self.state['all']\n",
    "        for i in range(self.agents):\n",
    "            for j in range(1, self.agents):\n",
    "                if (arr[10*i] > arr[10*j]) or ((arr[10*i] == arr[10*j]) and (arr[10*i+1] > arr[10*j+1])):\n",
    "                    self.do_swap(i, j)\n",
    "        \n",
    "    \n",
    "    def do_swap(self, a, b):\n",
    "        for i in range(10):\n",
    "            s = self.state['all'][10*a+i]\n",
    "            self.state['all'][10*a+i] = self.state['all'][10*b+i]\n",
    "            self.state['all'][10*b+i] = s\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Set starting state\n",
    "        self.state = self.observation_space.sample()\n",
    "        \"\"\"\n",
    "        image = self.state['grid']\n",
    "        walls = self.state['walls']\n",
    "        all = self.state['all']\n",
    "        for i in range(self.len):\n",
    "            for j in range(self.len):\n",
    "                image[i][j] = 1\n",
    "                walls[i][j] = 0\n",
    "        self.max_area = self.len * self.len-1 #change later\n",
    "        step = 2\n",
    "        for i in range(1, self.len-step+1, step):\n",
    "            for j in range(1, self.len-step+1, step):\n",
    "                x = i + random.randint(0, step) #wall generation\n",
    "                y = j + random.randint(0, step)\n",
    "                image[x][y] = 0\n",
    "                walls[x][y] = 1\n",
    "                self.max_area -= 1\n",
    "\n",
    "        for i in range(0, self.len-step, step):\n",
    "            for j in range(0, self.len-step, step):\n",
    "                x = i + random.randint(0, step)#blank generation\n",
    "                y = j + random.randint(0, step)\n",
    "                if image[x][y] == 1:\n",
    "                    self.max_area -= 1\n",
    "                image[x][y] = 0\n",
    "                x = i + random.randint(0, step)#blank generation\n",
    "                y = j + random.randint(0, step)\n",
    "                if image[x][y] == 1:\n",
    "                    self.max_area -= 1\n",
    "                image[x][y] = 0\n",
    "        \"\"\"\n",
    "        self.state['grid'][0] = np.array(grass_data, dtype = np.uint8)\n",
    "        self.state['grid'][1] = np.array(walls_data, dtype = np.uint8)\n",
    "        all = self.state['all']\n",
    "        for i in range(self.agents):# Set position\n",
    "            all[0+10*i] = corners[i][0]\n",
    "            all[1+10*i] = corners[i][1]\n",
    "\n",
    "            # Set border detect 0 = blocked, 1 = valid\n",
    "            all[2+10*i] = corners[i][0]\n",
    "            all[3+10*i] = 1 - corners[i][1]\n",
    "            all[4+10*i] = 1 - corners[i][0]\n",
    "            all[5+10*i] = corners[i][1]\n",
    "            all[6+10*i], all[7+10*i], all[8+10*i], all[9+10*i] = self.leastDist(all[0+10*i], all[1+10*i])\n",
    "\n",
    "        self.running_length = ((self.len) * (self.len) * (1 + self.agents)) // 2\n",
    "        self.area_clear = 0\n",
    "        self.max_area = np.sum(self.state['grid'][0])\n",
    "        return self.state, {}\n",
    "\n",
    "    def get_state(self):\n",
    "        output = []\n",
    "        for i in range(self.len):\n",
    "            output.append(np.append(self.state['all'][i*10:i*10+10], np.zeros(4)))\n",
    "        return (self.state['grid'], output)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Dict):\n",
    "        # We do not know features-dim here before going over all the items,\n",
    "        # so put something dummy for now. PyTorch requires calling\n",
    "        # nn.Module.__init__ before adding modules\n",
    "        super().__init__(observation_space, features_dim=1)\n",
    "\n",
    "        extractors = {}\n",
    "\n",
    "        total_concat_size = 0\n",
    "        # We need to know size of the output of this extractor,\n",
    "        # so go over all the spaces and compute output feature sizes\n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"grid\":\n",
    "                # We will just downsample one channel of the image by 4x4 and flatten.\n",
    "                # Assume the image is single-channel (subspace.shape[0] == 0)\n",
    "                extractors[key] = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), #try special max poolings plans\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "                total_concat_size += 4 * 4 * 64 #change later\n",
    "            else:\n",
    "                # Run through a simple MLP\n",
    "                extractors[key] = nn.Linear(subspace.shape[0], 64)\n",
    "                total_concat_size += 64\n",
    "\n",
    "        # Now concatenate the obtained features\n",
    "        self.extractors = nn.ModuleDict(extractors)\n",
    "        # Update the features dim manually\n",
    "        self._features_dim = total_concat_size\n",
    "\n",
    "    def forward(self, observations) -> th.Tensor:\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        # self.extractors contain nn.Modules that do all the processing.\n",
    "        for key, extractor in self.extractors.items():\n",
    "            input = observations[key]\n",
    "                #print(input.shape)\n",
    "            #print(observations[key])\n",
    "            #print(observations[key].shape)\n",
    "            thing = extractor(input)\n",
    "            encoded_tensor_list.append(thing)\n",
    "            #print(observations[key].shape)\n",
    "            #print(extractor(observations[key]))\n",
    "        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "        #print(encoded_tensor_list[0].shape)\n",
    "        #print(encoded_tensor_list[1].shape)\n",
    "        #print(encoded_tensor_list[2].shape)\n",
    "        return th.cat(encoded_tensor_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('all',\n",
       "               array([ 0,  0,  0,  1,  1,  0,  0,  1,  1,  0, 19, 19,  1,  0,  0,  1,  1,\n",
       "                       0,  0,  1, 19,  0,  1,  1,  0,  0,  0,  0,  1,  0,  0, 19,  0,  0,\n",
       "                       1,  1,  1,  1,  0,  0], dtype=uint8)),\n",
       "              ('grid',\n",
       "               array([[[0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
       "                       [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n",
       "                       [1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "                       [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "                       [0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "                       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0],\n",
       "                       [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
       "                       [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "                       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "                       [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "                       [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "                       [0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "                       [1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n",
       "                       [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "                       [1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0],\n",
       "                       [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "                       [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0],\n",
       "                       [0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "                       [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0],\n",
       "                       [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]],\n",
       "               \n",
       "                      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "                       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "                       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "                       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "                       [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "                       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "                       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "                       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "                       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "                       [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "                       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "                       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "                       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "                       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "                       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "                       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "                     dtype=uint8))]),\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env = MowerEnv(36, True)\n",
    "env = MowerEnv2([20, 4], True)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:38: UserWarning: It seems that your observation space grid is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:51: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.discrete.Discrete'>,) as action spaces but MultiDiscrete([4 4 4 4]) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39;49m\u001b[39mMultiInputPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, gamma \u001b[39m=\u001b[39;49m \u001b[39m0.999\u001b[39;49m,buffer_size\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, tau \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m, exploration_fraction\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m, exploration_final_eps\u001b[39m=\u001b[39;49m\u001b[39m0.15\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m111\u001b[39;49m, _init_setup_model \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,policy_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(features_extractor_class\u001b[39m=\u001b[39;49mCustomCombinedExtractor,activation_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mReLU, net_arch \u001b[39m=\u001b[39;49m [\u001b[39m128\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m32\u001b[39;49m]))\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:104\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     77\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     78\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[DQNPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    105\u001b[0m         policy,\n\u001b[0;32m    106\u001b[0m         env,\n\u001b[0;32m    107\u001b[0m         learning_rate,\n\u001b[0;32m    108\u001b[0m         buffer_size,\n\u001b[0;32m    109\u001b[0m         learning_starts,\n\u001b[0;32m    110\u001b[0m         batch_size,\n\u001b[0;32m    111\u001b[0m         tau,\n\u001b[0;32m    112\u001b[0m         gamma,\n\u001b[0;32m    113\u001b[0m         train_freq,\n\u001b[0;32m    114\u001b[0m         gradient_steps,\n\u001b[0;32m    115\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# No action noise\u001b[39;49;00m\n\u001b[0;32m    116\u001b[0m         replay_buffer_class\u001b[39m=\u001b[39;49mreplay_buffer_class,\n\u001b[0;32m    117\u001b[0m         replay_buffer_kwargs\u001b[39m=\u001b[39;49mreplay_buffer_kwargs,\n\u001b[0;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    119\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    120\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    121\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    122\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    123\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    124\u001b[0m         sde_support\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    125\u001b[0m         optimize_memory_usage\u001b[39m=\u001b[39;49moptimize_memory_usage,\n\u001b[0;32m    126\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(spaces\u001b[39m.\u001b[39;49mDiscrete,),\n\u001b[0;32m    127\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_initial_eps \u001b[39m=\u001b[39m exploration_initial_eps\n\u001b[0;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_final_eps \u001b[39m=\u001b[39m exploration_final_eps\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:110\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, use_sde_at_warmup, sde_support, supported_action_spaces)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     81\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     82\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[BasePolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ):\n\u001b[1;32m--> 110\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    111\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    112\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m    113\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    114\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    115\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    116\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    117\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    118\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    119\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49msupport_multi_env,\n\u001b[0;32m    120\u001b[0m         monitor_wrapper\u001b[39m=\u001b[39;49mmonitor_wrapper,\n\u001b[0;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    122\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m    123\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m    124\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m=\u001b[39m buffer_size\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vec_normalize_env \u001b[39m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[0;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.discrete.Discrete'>,) as action spaces but MultiDiscrete([4 4 4 4]) was provided"
     ]
    }
   ],
   "source": [
    "model = DQN(\"MultiInputPolicy\", env, gamma = 0.999,buffer_size=100000, verbose=1, tau = 0.1, batch_size = 64, exploration_fraction=0.95, exploration_final_eps=0.15, seed=111, _init_setup_model = True,policy_kwargs=dict(features_extractor_class=CustomCombinedExtractor,activation_fn=nn.ReLU, net_arch = [128, 64, 32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MultiInputPolicy\", env,gamma=0.999, learning_rate = 0.0001, verbose = 1, seed = 111, ent_coef = 0.001, policy_kwargs=dict(features_extractor_class=CustomCombinedExtractor, net_arch=dict(pi=[128, 64, 64], vf=[128, 64, 64]), activation_fn=nn.ReLU, optimizer_class = Adam, normalize_images=False)) #try and make activations not relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziduo\\Python\\Training\\Saved_Models\\new_thing\n"
     ]
    }
   ],
   "source": [
    "path2 = os.path.join('C:\\\\', 'Users', 'ziduo', 'Python',  'Training', 'Saved_Models', 'new_thing')\n",
    "print(path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39mload(path2, env )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path2' is not defined"
     ]
    }
   ],
   "source": [
    "model = PPO.load(path2, env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 526         |\n",
      "|    ep_rew_mean          | 1.41e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 700694      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022767905 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.778      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    value_loss           | 840         |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 526         |\n",
      "|    ep_rew_mean          | 1.42e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 704790      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018474547 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 535         |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | 0.007       |\n",
      "|    value_loss           | 6.27e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 526         |\n",
      "|    ep_rew_mean          | 1.39e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 708886      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012025721 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.42e+03    |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 3.31e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 1.42e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 712982      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020218913 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.717      |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 545         |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | -0.000927   |\n",
      "|    value_loss           | 4.02e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 1.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 717078      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020196162 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 85          |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | 0.000845    |\n",
      "|    value_loss           | 3.55e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 1.48e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 721174      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020069348 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.4e+03     |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | -8.73e-05   |\n",
      "|    value_loss           | 5.9e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 527         |\n",
      "|    ep_rew_mean          | 1.42e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 725270      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024215285 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.759      |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.56e+03    |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | 0.00684     |\n",
      "|    value_loss           | 5.12e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 1.39e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 729366      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011943245 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 98.6        |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 870         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 529         |\n",
      "|    ep_rew_mean          | 1.37e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 733462      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029258195 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.737      |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 199         |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 474         |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 1.4e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 737558      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029398594 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.3e+03     |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    value_loss           | 7.81e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 530        |\n",
      "|    ep_rew_mean          | 1.44e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 189        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 237        |\n",
      "|    total_timesteps      | 741654     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02578953 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.668     |\n",
      "|    explained_variance   | 0.489      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 971        |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | 0.00241    |\n",
      "|    value_loss           | 9.44e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 531         |\n",
      "|    ep_rew_mean          | 1.43e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 745750      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013784272 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.94e+03    |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | 0.00476     |\n",
      "|    value_loss           | 5.81e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 533         |\n",
      "|    ep_rew_mean          | 1.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 749846      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028412849 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.19e+03    |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | 0.00459     |\n",
      "|    value_loss           | 8.99e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 538         |\n",
      "|    ep_rew_mean          | 1.55e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 753942      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017289685 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.744      |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    value_loss           | 3.27e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 536        |\n",
      "|    ep_rew_mean          | 1.48e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 191        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 320        |\n",
      "|    total_timesteps      | 758038     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01970692 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.79      |\n",
      "|    explained_variance   | 0.663      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 667        |\n",
      "|    n_updates            | 3690       |\n",
      "|    policy_gradient_loss | -0.000274  |\n",
      "|    value_loss           | 7.08e+03   |\n",
      "----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.5e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 342         |\n",
      "|    total_timesteps      | 762134      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017574694 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.776      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    value_loss           | 1.26e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 535         |\n",
      "|    ep_rew_mean          | 1.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 766230      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014809929 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 201         |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.000947   |\n",
      "|    value_loss           | 1.38e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 535         |\n",
      "|    ep_rew_mean          | 1.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 770326      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016589284 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.781      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 394         |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    value_loss           | 2.06e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 405         |\n",
      "|    total_timesteps      | 774422      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036253367 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.59e+03    |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | 0.00118     |\n",
      "|    value_loss           | 1.04e+04    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 535         |\n",
      "|    ep_rew_mean          | 1.58e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 425         |\n",
      "|    total_timesteps      | 778518      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012949621 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.712      |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 156         |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | 0.00208     |\n",
      "|    value_loss           | 1.66e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 535         |\n",
      "|    ep_rew_mean          | 1.65e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 782614      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023887346 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.7e+03     |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | 0.00565     |\n",
      "|    value_loss           | 1.77e+04    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.79e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 465         |\n",
      "|    total_timesteps      | 786710      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014164286 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.69       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.64e+03    |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | -0.000341   |\n",
      "|    value_loss           | 3.57e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.76e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 790806      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019491754 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.27e+03    |\n",
      "|    n_updates            | 3850        |\n",
      "|    policy_gradient_loss | 0.00174     |\n",
      "|    value_loss           | 1.96e+04    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 533         |\n",
      "|    ep_rew_mean          | 1.75e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 794902      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022755926 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 524         |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.00421     |\n",
      "|    value_loss           | 6.73e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 531         |\n",
      "|    ep_rew_mean          | 1.73e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 798998      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009459319 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.000914   |\n",
      "|    value_loss           | 2.86e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 532         |\n",
      "|    ep_rew_mean          | 1.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 543         |\n",
      "|    total_timesteps      | 803094      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020142697 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 446         |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | 0.000402    |\n",
      "|    value_loss           | 1.83e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 532        |\n",
      "|    ep_rew_mean          | 1.7e+03    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 196        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 563        |\n",
      "|    total_timesteps      | 807190     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03236182 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.675     |\n",
      "|    explained_variance   | 0.595      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 611        |\n",
      "|    n_updates            | 3930       |\n",
      "|    policy_gradient_loss | 0.00908    |\n",
      "|    value_loss           | 8.22e+03   |\n",
      "----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.73e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 582         |\n",
      "|    total_timesteps      | 811286      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014121117 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 875         |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | -0.000632   |\n",
      "|    value_loss           | 4.79e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 535         |\n",
      "|    ep_rew_mean          | 1.72e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 815382      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015284281 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 249         |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | 5.45e-05    |\n",
      "|    value_loss           | 2.36e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.75e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 621         |\n",
      "|    total_timesteps      | 819478      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020343922 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.37e+03    |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | 0.00154     |\n",
      "|    value_loss           | 1.84e+04    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.72e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 823574      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017977513 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.749      |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 434         |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    value_loss           | 784         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.71e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 660         |\n",
      "|    total_timesteps      | 827670      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022007803 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 536         |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    value_loss           | 4.07e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 532        |\n",
      "|    ep_rew_mean          | 1.62e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 198        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 679        |\n",
      "|    total_timesteps      | 831766     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01617559 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.732     |\n",
      "|    explained_variance   | 0.326      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.29e+03   |\n",
      "|    n_updates            | 4050       |\n",
      "|    policy_gradient_loss | 0.00702    |\n",
      "|    value_loss           | 8.28e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 533         |\n",
      "|    ep_rew_mean          | 1.6e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 835862      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015874501 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.726      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 239         |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    value_loss           | 1.25e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 534         |\n",
      "|    ep_rew_mean          | 1.64e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 719         |\n",
      "|    total_timesteps      | 839958      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027471822 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 340         |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    value_loss           | 6.87e+03    |\n",
      "-----------------------------------------\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 532         |\n",
      "|    ep_rew_mean          | 1.7e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 844054      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019175842 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.49e+03    |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    value_loss           | 6.09e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m2000000\u001b[39;49m, log_interval \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[1;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 272\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    273\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    274\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=2000000, log_interval = 2, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#log_path2 = os.path.join('Training', 'Saved Models', 'PPO_thing')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#path2 = os.path.join('C:', 'Users', 'ziduo', 'Python',  'Training', 'Saved_Models', 'new_thing')\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m./models/saved_test.zip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:845\u001b[0m, in \u001b[0;36mBaseAlgorithm.save\u001b[1;34m(self, path, exclude, include)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[39m# Build dict of state_dicts\u001b[39;00m\n\u001b[0;32m    843\u001b[0m params_to_save \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_parameters()\n\u001b[1;32m--> 845\u001b[0m save_to_zip_file(path, data\u001b[39m=\u001b[39;49mdata, params\u001b[39m=\u001b[39;49mparams_to_save, pytorch_variables\u001b[39m=\u001b[39;49mpytorch_variables)\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:311\u001b[0m, in \u001b[0;36msave_to_zip_file\u001b[1;34m(save_path, data, params, pytorch_variables, verbose)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39m# data/params can be None, so do not\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m# try to serialize them blindly\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     serialized_data \u001b[39m=\u001b[39m data_to_json(data)\n\u001b[0;32m    313\u001b[0m \u001b[39m# Create a zip-archive and write our objects there.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(save_path, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m archive:\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Do not try to save \"None\" elements\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:99\u001b[0m, in \u001b[0;36mdata_to_json\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     92\u001b[0m     serializable_data[data_key] \u001b[39m=\u001b[39m data_item\n\u001b[0;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[39m# Not serializable, cloudpickle it into\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39m# bytes and convert to base64 string for storing.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Also store type of the class for consumption\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# from other languages/humans, so we have an\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39m# idea what was being stored.\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     base64_encoded \u001b[39m=\u001b[39m base64\u001b[39m.\u001b[39mb64encode(cloudpickle\u001b[39m.\u001b[39;49mdumps(data_item))\u001b[39m.\u001b[39mdecode()\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Use \":\" to make sure we do\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# not override these keys\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[39m# when we include variables of the object later\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     cloudpickle_serialization \u001b[39m=\u001b[39m {\n\u001b[0;32m    105\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m:type:\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(data_item)),\n\u001b[0;32m    106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m:serialized:\u001b[39m\u001b[39m\"\u001b[39m: base64_encoded,\n\u001b[0;32m    107\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     70\u001b[0m     cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m         file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m     )\n\u001b[1;32m---> 73\u001b[0m     cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:563\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[0;32m    562\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m         \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[0;32m    564\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    565\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrecursion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:653\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[39mreturn\u001b[39;00m _class_reduce(obj)\n\u001b[0;32m    652\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mFunctionType):\n\u001b[1;32m--> 653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_reduce(obj)\n\u001b[0;32m    654\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    655\u001b[0m     \u001b[39m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[39m# distpatch_table\u001b[39;00m\n\u001b[0;32m    657\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:526\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m    525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 526\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dynamic_function_reduce(obj)\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:507\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reduce a function that is not pickleable via attribute lookup.\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m newargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_getnewargs(func)\n\u001b[1;32m--> 507\u001b[0m state \u001b[39m=\u001b[39m _function_getstate(func)\n\u001b[0;32m    508\u001b[0m \u001b[39mreturn\u001b[39;00m (types\u001b[39m.\u001b[39mFunctionType, newargs, state, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m         _function_setstate)\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_function_getstate\u001b[39m(func):\n\u001b[0;32m    140\u001b[0m     \u001b[39m# - Put func's dynamic attributes (stored in func.__dict__) in state. These\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m#   attributes will be restored at unpickling time using\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39m#   unpickling time by iterating over slotstate and calling setattr(func,\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[39m#   slotname, slotvalue)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     slotstate \u001b[39m=\u001b[39m {\n\u001b[0;32m    147\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[0;32m    148\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m__closure__\u001b[39m\u001b[39m\"\u001b[39m: func\u001b[39m.\u001b[39m\u001b[39m__closure__\u001b[39m,\n\u001b[0;32m    155\u001b[0m     }\n\u001b[1;32m--> 157\u001b[0m     f_globals_ref \u001b[39m=\u001b[39m _extract_code_globals(func\u001b[39m.\u001b[39;49m\u001b[39m__code__\u001b[39;49m)\n\u001b[0;32m    158\u001b[0m     f_globals \u001b[39m=\u001b[39m {k: func\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f_globals_ref \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m\n\u001b[0;32m    159\u001b[0m                  func\u001b[39m.\u001b[39m\u001b[39m__globals__\u001b[39m}\n\u001b[0;32m    161\u001b[0m     closure_values \u001b[39m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(_get_cell_contents, func\u001b[39m.\u001b[39m\u001b[39m__closure__\u001b[39m))\n\u001b[0;32m    163\u001b[0m         \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__closure__\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ()\n\u001b[0;32m    164\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:236\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[1;34m(co)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m out_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     names \u001b[39m=\u001b[39m co\u001b[39m.\u001b[39mco_names\n\u001b[1;32m--> 236\u001b[0m     out_names \u001b[39m=\u001b[39m {names[oparg] \u001b[39mfor\u001b[39;49;00m _, oparg \u001b[39min\u001b[39;49;00m _walk_global_ops(co)}\n\u001b[0;32m    238\u001b[0m     \u001b[39m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# syntax generates a constant code object corresonding to the one\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[39m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[39m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# add the result to code_globals\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m co\u001b[39m.\u001b[39mco_consts:\n",
      "File \u001b[1;32mc:\\Users\\ziduo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:236\u001b[0m, in \u001b[0;36m<setcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m out_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     names \u001b[39m=\u001b[39m co\u001b[39m.\u001b[39mco_names\n\u001b[1;32m--> 236\u001b[0m     out_names \u001b[39m=\u001b[39m {names[oparg] \u001b[39mfor\u001b[39;00m _, oparg \u001b[39min\u001b[39;00m _walk_global_ops(co)}\n\u001b[0;32m    238\u001b[0m     \u001b[39m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# syntax generates a constant code object corresonding to the one\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[39m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[39m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# add the result to code_globals\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m co\u001b[39m.\u001b[39mco_consts:\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "#log_path2 = os.path.join('Training', 'Saved Models', 'PPO_thing')\n",
    "#path2 = os.path.join('C:', 'Users', 'ziduo', 'Python',  'Training', 'Saved_Models', 'new_thing')\n",
    "model.save('./models/saved_test.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "547\n",
      "4873.705318565668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "best = 0 #around 19% success rate\n",
    "low = 100000\n",
    "for i in range(500):\n",
    "    obs, _ = env.reset()\n",
    "    average = 0\n",
    "    length = 0\n",
    "    while True:\n",
    "        # Next action:\n",
    "        # (feed the observation to your agent here)\n",
    "        action = model.predict(obs)\n",
    "\n",
    "        # Processing:\n",
    "        obs, reward, terminated, _, info = env.step(action[0])\n",
    "        average += reward\n",
    "        length += 1\n",
    "        # Rendering the game:\n",
    "        # (remove this two lines during training)\n",
    "        #env.render()\n",
    "        #time.sleep(1 / 10)  # FPS\n",
    "        \n",
    "        # Checking if the player is still alive\n",
    "        if terminated:\n",
    "            break\n",
    "    best = max(best, average)\n",
    "    if average > 3000:\n",
    "        low = min(low, length)\n",
    "print(length)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('all',\n",
       "               array([ 0,  0,  0,  1,  1,  0,  0,  1,  1,  0, 19, 19,  1,  0,  0,  1,  1,\n",
       "                       0,  0,  1, 19,  0,  1,  1,  0,  0,  0,  0,  1,  0,  0, 19,  0,  0,\n",
       "                       1,  1,  1,  1,  0,  0], dtype=uint8)),\n",
       "              ('grid',\n",
       "               array([[[0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
       "                       [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n",
       "                       [1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "                       [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "                       [0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "                       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0],\n",
       "                       [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
       "                       [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "                       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "                       [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "                       [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "                       [0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "                       [1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n",
       "                       [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "                       [1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0],\n",
       "                       [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "                       [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0],\n",
       "                       [0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "                       [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0],\n",
       "                       [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]],\n",
       "               \n",
       "                      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "                       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "                       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "                       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "                       [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "                       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "                       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "                       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "                       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "                       [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "                       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "                       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "                       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "                       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "                       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "                       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "                     dtype=uint8))]),\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate = MowerEnv2([20, 4], True)\n",
    "simulate.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m                 made\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Processing:\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m obs, reward, terminated, _, info \u001b[39m=\u001b[39m simulate\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     27\u001b[0m \u001b[39m# Rendering the game:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m# (remove this two lines during training)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[39m# Checking if the player is still alive\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m terminated:\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mMowerEnv2.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mall\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents):\n\u001b[0;32m     26\u001b[0m     \u001b[39m# Apply action (no walls for now)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m[\u001b[39m0\u001b[39m \u001b[39m+\u001b[39m j \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirect[action[j]][\u001b[39m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m[\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m j \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirect[action[j]][\u001b[39m1\u001b[39m]\n\u001b[0;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misValid(x, y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "obs, _ = simulate.reset()\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    made = False\n",
    "    simulate.render()\n",
    "    time.sleep(1 / 60)  # FPS\n",
    "    while not made:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    action = 0\n",
    "                    made= True\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    action = 1\n",
    "                    made= True\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    action = 2\n",
    "                    made= True\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    action = 3\n",
    "                    made= True\n",
    "                \n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, _, info = simulate.step(action)\n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    \n",
    "    # Checking if the player is still alive\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1,1,2,3,2,2,2,2,2,0,2,0,2,2,1,2,1,1,1,2,3,2,1,2,1,1,0,1,1,0,1,0,0,1,1,0,3,3,1,0,0,1,2,3,1,2,0,2,1,0,1,1,0,1,0,3,2,2,2,2,1,2,2,2,2,1,2,2,2,2,3,3,2,2,3,2,0,2,2,2,2,2,2,1,1,2,3,1,1,3,1,3,1,2,1,1,1,0,1,3,1,1,3,0,1,0,1,0,0,1,1,1,1,2,1,2,1,1,2,1,3,3,2,3,2,3,2,2,1,1,1,3,1,1,0,2,2,2,2,2,3,0,3,3,0,3,0,2,3,3,0,2,1,2,2,1,2,2,1,2,1,1,1,1,1,2,1,2,2,1,0,1,1,2,0,0,0,0,0,0,0,2,3,2,2,2,2,0,1,3,2,0,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,2,0,1,3,1,3,1,3,1,0,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,2,1,3,1,0,3,1,3,1,3,1,3,1,3,1,3,1,0,3,1,3,1,3,1,3,1,3,2,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,3,1,3,1,3,1,3,1,2,3,1,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,2,1,3,0,1,3,1,2,0,3,1,3,1,3,2,0,2,0,0,1,3,1,3,0,1,3,1,3,1,3,1,3,1,3,1,3,0,1,3,1,0,3,1,3,1,3,1,3,1,3,1,0,0,0,3,3,3,1,0,3,0,0,0,0,0,0,0,3,1,3,0,0,0,3,3,2,2,2,2,3,3,0,3,1,1,0,2,2,2,2,2,0,2,2,2,2,2,1,2,1,2,2,1,2,2,1,1,2,0,0,0,2,0,2,0,3,0,1,2,0,3,0,0,3,0,3,3,2,0,3,2,0,2,0,2,0,2,3,2,0,2,0,2,0,0,2,0,2,3,0,0,0,3,3,0,2,0,3,3,3,2,2,3,2,0,3,3,2,3,3,3,0,2,2,1,3,0,2,3,2,2,2,3,2,0,3,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,3,3,3,2,3,3,3,2,2,2,1,2,3,1,2,3,2,3,3,3,3,3,3,3,0,3,0,0,0,0,1,1,2,1,2,2,1,2,1,1,0,2,3,2,0,2,0,3,0,0,1,0,1,2,0,0,3,3,2,0,3,0,3,0,1,3,0,0,0,3,2,3,2,2,1,1,0,0,1,1,2,2,1,0,1,2,3,0,1,1,1,1,0,0,3,1,3,3,1,0,1,1,1,0,1,0,0,0,3,3,2,2,0,0,1,1,1,0,1,2,1,0,2,2,2,3,2,3,2,0,2,0,2,3,0,2,0,2,0,2,3,0,2,0,0,2,0,2,0,1,3,2,0,2,0,2,0,2,2,0,2,0,2,0,0,2,0,2,0,2,2,0,2,0,2,0,0,2,0,2,0,2,0,2,0,2,0,2,2,0,2,3,0,2,0,2,0,2,2,3,0,2,0,3,0,2,0,2,0,3,2,0,2,2,3,3,0,2,3,2,3,2,3,2,3,2,2,1,2,2,3,1,3,3,3,2,1,2,2,2,3,1,1,3,1,3,1,2,1,1,0,1,2,2,2,2,1,1,0,2,1,3,1,1,1,0,0,0,0,0,0,0,0,0,3,0,3,2,1,0,0,3,2,0,0,1,0,1,0,2,1,1,0,0,2,0,2,0,2,0,2,0,2,0,2,1,0,2,0,2,0,2,0,2,1,3,0,2,2,2,2,1,3,0,0,0,1,0,1,2,0,0,1,1,1,1,0,0,2,0,1,1,3,2,2,0,2,1,0,0,2,0,1,3,2,2,2,0,0,2,0,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,1,3,2,0,2,0,2,0,2,0,2,0,0,2,0,2,0,2,0,2,3,0,2,0,2,0,2,0,0,2,0,2,0,2,0,2,0,2,0,2,0,2,0,2,1,0,3,1,1,3,2,0,2,0,2,0,2,0,2,0,2,0,3,2,0,0,1,0,0,0,3,2,2,0,0,2,0,0,2,0,1,0,2,1,3,3,3,3,3,0,0,1,0,0,3,0,3,2,2,1,1,1,1,1,0,1,0,0,0,1,0,0,1,2,0,0,1,0,1,1,0,0,1,1,0,1,0,0,3,3,3,1,3,1,3,3,3,3,3,3,2,3,0,3,3,3,3,3,3,3,3,2,0,3,3,3,2,2,2,3,1,0,1,0,1,1,1,1,1,2,2,2,2,0,2,3,1,2,1,1,2,1,0,1,3,2,1,3,0,1,0,0,0,0,3,3,2,1,1,1,2,0,2,1,0,2,1,2,3,3,0,1,2,2,3,1,3,1,3,1,3,0,1,2,1,0,0,0,0,3,0,1,3,1,1,0,1,0,2,0,2,3,2,0,1,3,1,3,1,2,0,3,1,3,1,3,1,3,0,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,0,1,2,3,1,3,1,3,1,3,1,3,1,2,0,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,3,1,1,3,1,3,1,3,2,0,1,3,2,0,1,3,1,3,1,3,1,3,0,0,1,3,1,3,1,3,3,1,1,3,0,1,3,1,1,1,2,3,3,3,3,2,2,2,3,0,2,2,1,3,2,3,2,2,2,2,3,0,0,3,3,0,1,3,1,3,1,3,1,3,1,3,1,3,1,3,3,1,3,3,1,3,1,3,1,3,2,3,3,3,3,3,3,3,3,3,3,2,3,1,2,2,2,2,2,0,2,2,2,2,3,1,3,3,1,2,3,3,2,3,2,3,1,3,2,0,0,3,3,0,3,3,2,0,3,3,3,0,3,2,3,2,2,1,3,3,1,0,0,0,0,3,3,2,3,0,3,0,0,2,0,1,0,3,1,3,0,2,3,3,3,0,0,2,0,0,1,0,0,3,0,3,3,2,3,2,2,2,2,0,0,1,3,2,2,2,0,2,0,2,1,1,2,1,3,1,0,3,2,1,3,2,3,3,1,2,2,2,1,2,1,0,1,2,1,3,3,1,2,2,1,1,0,2,3,2,3,1,3,1,3,1,3,1,3,1,2,2,3,2,3,1,3,1,3,1,3,0,1,1,3,1,2,3,1,3,3,1,1,0,1,0,2,3,1,2,3,0,0,3,0,2,0,0,0,3,0,3,3,3,2,2,2,0,2,0,2,0,2,0,0,2,2,0,2,0,2,0,2,2,1,1,2,0,1,1,1,1,2,0,1,1,1,2,0,1,2,0,1,3,0,0,2,0,3,0,1,3,2,0,3,1,3,1,3,1,3,1,3,1,3,0,1,1,1,0,0,1,1,1,1,2,0,3,1,2,0,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,3,1,1,0,0,0,2,3,1,3,1,3,1,3,3,1,3,1,3,1,3,1,1,3,0,3,1,1,3,1,3,1,3,1,1,3,1,3,1,3,3,1,3,1,3,1,3,3,3,1,2,3,0,1,3,1,1,1,3,1,3,2,0,1,3,1,3,0,0,1,0,0,3,0,2,3,3,0,0,1,3,0,2,3,3,1,1,2,0,1,0,1,2,2,0,3,0,0,3,0,0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "546.0\n"
     ]
    }
   ],
   "source": [
    "print(len(list)-36*36)\n",
    "print(np.sum(grass_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list[0] is not a generic class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Processing:\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m obs, reward, terminated, _, info \u001b[39m=\u001b[39m simulate\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Rendering the game:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# (remove this two lines during training)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39m# Checking if the player is still alive\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m terminated:\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mMowerEnv2.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mall\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents):\n\u001b[0;32m     26\u001b[0m     \u001b[39m# Apply action (no walls for now)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m[\u001b[39m0\u001b[39m \u001b[39m+\u001b[39m j \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirect[action[j]][\u001b[39m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m[\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m j \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdirect[action[j]][\u001b[39m1\u001b[39m]\n\u001b[0;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misValid(x, y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: list[0] is not a generic class"
     ]
    }
   ],
   "source": [
    "obs, _ = simulate.reset()\n",
    "step = 0\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    made = False\n",
    "    simulate.render()\n",
    "    time.sleep(1 / 60)  # FPS\n",
    "    action = list[step]\n",
    "    step +=1\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, _, info = simulate.step(action)\n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    \n",
    "    # Checking if the player is still alive\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "lesgo\n",
      "joyce qu\n",
      "117\n",
      "3260\n"
     ]
    }
   ],
   "source": [
    "dir = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n",
    "obs, _ = simulate.reset()\n",
    "step = 0\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    made = False\n",
    "    action = []\n",
    "    for i in range(4):\n",
    "        calc = np.zeros((20, 20))\n",
    "        list = deque()\n",
    "        list.append([obs['all'][0+10*i], obs['all'][1+10*i]])\n",
    "        calc[obs['all'][0+10*i]][obs['all'][1+10*i]] = 1\n",
    "        move = -1\n",
    "        while True:\n",
    "            tuple = list.popleft()\n",
    "            for i in range(4):\n",
    "                x = tuple[0] + dir[i][0]\n",
    "                y = tuple[1] + dir[i][1]\n",
    "                if x >= 0 and x < 20 and y >=0 and y < 20:\n",
    "                    if obs['grid'][1][x][y] == 0 and calc[x][y] == 0:\n",
    "                        calc[x][y] = 1\n",
    "                        choice = i\n",
    "                        if len(tuple) == 3:\n",
    "                            choice = tuple[2]\n",
    "                        if obs['grid'][0][x][y] == 0:\n",
    "                            ret = [x, y, choice]\n",
    "                            list.append(ret)\n",
    "                        else:\n",
    "                            move = choice\n",
    "                            break\n",
    "            if move != -1:\n",
    "                break\n",
    "        action.append(move)\n",
    "        \n",
    "    \n",
    "    time.sleep(1 / 30)  # FPS\n",
    "    \n",
    "    step +=1\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, _, info = simulate.step(action)\n",
    "    simulate.render()\n",
    "    # Rendering the game:\n",
    "    # (remove this two lines during training)\n",
    "    \n",
    "    # Checking if the player is still alive\n",
    "    if terminated:\n",
    "        break\n",
    "print(step)\n",
    "print(reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "ret = [5, 6]\n",
    "if len(ret) == 2:\n",
    "    ret += [2]\n",
    "print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
